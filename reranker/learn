#!/usr/bin/env python
import itertools
import math
import optparse
import random
import sys
import bleu
from collections import namedtuple

# Simplified implementation of Pairwise Ranking Optimization (PRO) due to
# Hopkins & May (2011): http://aclweb.org/anthology-new/D/D11/D11-1125.pdf
#
# Differences from paper:
# -Does not include candidate generation (candidates are assumed to be given)
# -Uses perceptron instead of maxent for classification

translation_candidate = namedtuple("candidate", "features, smoothed_bleu")
optparser = optparse.OptionParser()
optparser.add_option("-r", "--reference", dest="reference", default="data/train.en", help="English reference sentences")
optparser.add_option("-n", "--nbest", dest="nbest", default="data/train.nbest", help="N-best lists")
optparser.add_option("-m", "--num-training-sentences", dest="m", default=sys.maxint, type="int", help="Number of training sentences (default=all)")
optparser.add_option("-t", "--tau", dest="tau", default=5000, type="int", help="PRO samples per input sentence (tau, default=5000)")
optparser.add_option("-x", "--xi", dest="xi", default=50, type="int", help="PRO training instances per input sentence (xi, default=50)")
optparser.add_option("-e", "--eta", dest="eta", default=0.1, type="float", help="Perceptron learning rate (eta, default=0.1)")
optparser.add_option("-a", "--alpha", dest="alpha", default=0.05, type="float", help="Sampler acceptance cutoff (alpha, default=0.05)")
optparser.add_option("-i", "--epochs", dest="epochs", default=5, type="int", help="Perceptron epochs (default=10)")
optparser.add_option("-s", "--random-seed", dest="seed", default="0", type="string", help="Random number seed (default='0')")
(opts,_) = optparser.parse_args()

ref = [line.strip().split() for line in open(opts.reference)][:opts.m]

sys.stderr.write("Reading N-best lists...")
nbests = [[] for _ in ref]
for n, line in enumerate(open(opts.nbest)):
  (i, sentence, features) = line.strip().split("|||")
  (i, features) = (int(i), [float(h) for h in features.strip().split()])
  if i >= len(ref):
    break
  stats = tuple(bleu.bleu_stats(sentence.strip().split(), ref[i]))
  nbests[i].append(translation_candidate(features, bleu.smoothed_bleu(stats)))
  if n % 2000 == 0:
    sys.stderr.write(".")
sys.stderr.write("\n")

w = [0.0 for _ in xrange(len(nbests[0][0].features))]
random.seed(opts.seed)

# PRO perceptron. Mostly follows the notation of Hopkins & May.
for i in xrange(opts.epochs):
  sys.stderr.write("Epoch %d..." % i)
  (observations, errors) = (0, 0.0)
  for nbest in nbests:
    def V():
      for _ in xrange(opts.tau):
        c1 = random.choice(nbest)
        c2 = random.choice(nbest)
        if c1 != c2 and math.fabs(c1.smoothed_bleu - c2.smoothed_bleu) > opts.alpha:
          yield (c1, c2) if c1.smoothed_bleu > c2.smoothed_bleu else (c2, c1)
    for c1, c2 in sorted(V(), key=lambda (c1,c2): c2.smoothed_bleu-c1.smoothed_bleu)[:opts.xi]: # Figure 4 sampling algorithm
      x = [c1j-c2j for c1j,c2j in zip(c1.features, c2.features)]
      if sum([xj*wj for xj,wj in zip(x,w)]) <= 0:
        errors += 1
        w = [(opts.eta*xj)+wj for xj,wj in zip(x,w)]
      observations += 1
      if observations % 2000 == 0:
        sys.stderr.write(".")
  sys.stderr.write("classification error rate: %f (%d observations)\n" % (errors/observations, observations))

print "\n".join([str(weight) for weight in w])
